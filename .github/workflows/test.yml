name: HomePro Scraper Tests

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  workflow_dispatch:

env:
  PYTHON_VERSION: '3.11'
  NODE_VERSION: '18'

jobs:
  # Job 1: Code Quality and Security
  quality:
    name: Code Quality & Security
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Cache Python dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install bandit safety flake8 mypy black isort
    
    - name: Code formatting check (Black)
      run: black --check --diff app tests
    
    - name: Import sorting check (isort)
      run: isort --check-only --diff app tests
    
    - name: Lint with flake8
      run: |
        flake8 app tests --count --select=E9,F63,F7,F82 --show-source --statistics
        flake8 app tests --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics
    
    - name: Type checking with mypy
      run: mypy app --ignore-missing-imports --no-strict-optional
    
    - name: Security scan with bandit
      run: bandit -r app -f json -o bandit-report.json || true
    
    - name: Upload bandit report
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: security-report
        path: bandit-report.json
    
    - name: Check dependencies for vulnerabilities
      run: safety check --json --output safety-report.json || true
    
    - name: Upload safety report
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: dependency-security-report
        path: safety-report.json

  # Job 2: Unit Tests
  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest
    needs: quality
    
    strategy:
      matrix:
        python-version: ['3.9', '3.10', '3.11']
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Cache Python dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ matrix.python-version }}-${{ hashFiles('**/requirements.txt') }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-asyncio pytest-cov pytest-xdist
    
    - name: Create test environment file
      run: |
        cat > .env.test << EOF
        SUPABASE_URL=https://test.supabase.co
        SUPABASE_ANON_KEY=test-anon-key
        SUPABASE_SERVICE_ROLE_KEY=test-service-key
        POSTGRES_URL=postgresql://test:test@localhost/test
        POSTGRES_PASSWORD=test
        FIRECRAWL_API_KEY=fc-test-key
        ENVIRONMENT=test
        LOG_LEVEL=debug
        EOF
    
    - name: Run unit tests with coverage
      env:
        TESTING: true
        ENV_FILE: .env.test
      run: |
        python -m pytest tests/unit/ \
          --cov=app \
          --cov-report=xml:coverage.xml \
          --cov-report=html:htmlcov \
          --cov-report=term-missing \
          --junitxml=junit.xml \
          -v \
          --tb=short
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unit-tests
        name: codecov-umbrella
        fail_ci_if_error: false
    
    - name: Upload test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: unit-test-results-${{ matrix.python-version }}
        path: |
          junit.xml
          htmlcov/
          coverage.xml

  # Job 3: Integration Tests
  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    needs: unit-tests
    
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: test_homepro
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
      
      redis:
        image: redis:7
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-asyncio pytest-cov
    
    - name: Wait for services
      run: |
        sleep 10
        pg_isready -h localhost -p 5432 -U postgres
        redis-cli -h localhost -p 6379 ping
    
    - name: Set up test database
      env:
        PGPASSWORD: postgres
      run: |
        psql -h localhost -U postgres -d test_homepro -f scripts/create_schema.sql
    
    - name: Create integration test environment
      run: |
        cat > .env.integration << EOF
        SUPABASE_URL=https://test.supabase.co
        SUPABASE_ANON_KEY=test-anon-key
        SUPABASE_SERVICE_ROLE_KEY=test-service-key
        POSTGRES_URL=postgresql://postgres:postgres@localhost:5432/test_homepro
        POSTGRES_PASSWORD=postgres
        FIRECRAWL_API_KEY=fc-test-key
        ENVIRONMENT=test
        LOG_LEVEL=debug
        REDIS_URL=redis://localhost:6379/0
        EOF
    
    - name: Run integration tests
      env:
        TESTING: true
        ENV_FILE: .env.integration
      run: |
        python -m pytest tests/integration/ \
          --cov=app \
          --cov-append \
          --cov-report=xml:integration-coverage.xml \
          --junitxml=integration-junit.xml \
          -v \
          --tb=short \
          -m "integration"
    
    - name: Upload integration test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: integration-test-results
        path: |
          integration-junit.xml
          integration-coverage.xml

  # Job 4: End-to-End Tests
  e2e-tests:
    name: End-to-End Tests
    runs-on: ubuntu-latest
    needs: integration-tests
    
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: test_homepro
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Set up Node.js
      uses: actions/setup-node@v4
      with:
        node-version: ${{ env.NODE_VERSION }}
        cache: 'npm'
        cache-dependency-path: frontend/package-lock.json
    
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-asyncio selenium
    
    - name: Install Chrome
      run: |
        wget -q -O - https://dl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
        sudo sh -c 'echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" >> /etc/apt/sources.list.d/google-chrome.list'
        sudo apt-get update
        sudo apt-get install -y google-chrome-stable
    
    - name: Install ChromeDriver
      run: |
        CHROME_VERSION=$(google-chrome --version | cut -d ' ' -f3 | cut -d '.' -f1)
        wget -q "https://chromedriver.storage.googleapis.com/LATEST_RELEASE_${CHROME_VERSION}" -O chrome_version
        CHROMEDRIVER_VERSION=$(cat chrome_version)
        wget -q "https://chromedriver.storage.googleapis.com/${CHROMEDRIVER_VERSION}/chromedriver_linux64.zip"
        unzip chromedriver_linux64.zip
        sudo chmod +x chromedriver
        sudo mv chromedriver /usr/local/bin/
    
    - name: Install frontend dependencies
      run: |
        cd frontend
        npm ci
    
    - name: Build frontend
      run: |
        cd frontend
        npm run build
    
    - name: Set up test database
      env:
        PGPASSWORD: postgres
      run: |
        psql -h localhost -U postgres -d test_homepro -f scripts/create_schema.sql
    
    - name: Create E2E test environment
      run: |
        cat > .env.e2e << EOF
        SUPABASE_URL=https://test.supabase.co
        SUPABASE_ANON_KEY=test-anon-key
        SUPABASE_SERVICE_ROLE_KEY=test-service-key
        POSTGRES_URL=postgresql://postgres:postgres@localhost:5432/test_homepro
        POSTGRES_PASSWORD=postgres
        FIRECRAWL_API_KEY=fc-test-key
        ENVIRONMENT=test
        LOG_LEVEL=info
        EOF
    
    - name: Start backend server
      env:
        ENV_FILE: .env.e2e
      run: |
        cd /
        python -m uvicorn app.api.main:app --host 0.0.0.0 --port 8000 &
        sleep 10
    
    - name: Start frontend server
      run: |
        cd frontend
        npm start &
        sleep 30
    
    - name: Wait for services to be ready
      run: |
        timeout 60 bash -c 'until curl -f http://localhost:8000/api/products/filters/brands; do sleep 2; done'
        timeout 60 bash -c 'until curl -f http://localhost:3000; do sleep 2; done'
    
    - name: Run E2E tests
      env:
        TESTING: true
        ENV_FILE: .env.e2e
      run: |
        python -m pytest tests/e2e/ \
          --junitxml=e2e-junit.xml \
          -v \
          --tb=short \
          -m "e2e and not external" \
          --maxfail=3
    
    - name: Upload E2E test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: e2e-test-results
        path: |
          e2e-junit.xml
    
    - name: Upload screenshots on failure
      uses: actions/upload-artifact@v3
      if: failure()
      with:
        name: e2e-screenshots
        path: "*.png"

  # Job 5: Performance Tests
  performance-tests:
    name: Performance Tests
    runs-on: ubuntu-latest
    needs: integration-tests
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-asyncio locust
    
    - name: Run performance tests
      run: |
        # Add performance test commands here
        # Example: locust --headless --users 10 --spawn-rate 2 -t 30s --host http://localhost:8000
        echo "Performance tests would run here"
    
    - name: Upload performance results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: performance-results
        path: performance-results/

  # Job 6: Security Tests
  security-tests:
    name: Security Tests
    runs-on: ubuntu-latest
    needs: quality
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-asyncio
    
    - name: Run security tests
      env:
        TESTING: true
      run: |
        python -m pytest tests/ \
          -m "security" \
          --junitxml=security-junit.xml \
          -v
    
    - name: Upload security test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: security-test-results
        path: security-junit.xml

  # Job 7: Test Summary
  test-summary:
    name: Test Summary
    runs-on: ubuntu-latest
    needs: [quality, unit-tests, integration-tests, e2e-tests, security-tests]
    if: always()
    
    steps:
    - name: Download all artifacts
      uses: actions/download-artifact@v3
    
    - name: Display test summary
      run: |
        echo "## Test Results Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        if [ -f unit-test-results-*/junit.xml ]; then
          echo "✅ Unit tests completed" >> $GITHUB_STEP_SUMMARY
        else
          echo "❌ Unit tests failed" >> $GITHUB_STEP_SUMMARY
        fi
        
        if [ -f integration-test-results/integration-junit.xml ]; then
          echo "✅ Integration tests completed" >> $GITHUB_STEP_SUMMARY
        else
          echo "❌ Integration tests failed" >> $GITHUB_STEP_SUMMARY
        fi
        
        if [ -f e2e-test-results/e2e-junit.xml ]; then
          echo "✅ E2E tests completed" >> $GITHUB_STEP_SUMMARY
        else
          echo "❌ E2E tests failed" >> $GITHUB_STEP_SUMMARY
        fi
        
        if [ -f security-test-results/security-junit.xml ]; then
          echo "✅ Security tests completed" >> $GITHUB_STEP_SUMMARY
        else
          echo "❌ Security tests failed" >> $GITHUB_STEP_SUMMARY
        fi
    
    - name: Fail if critical tests failed
      run: |
        if [ ! -f unit-test-results-*/junit.xml ] || [ ! -f integration-test-results/integration-junit.xml ]; then
          echo "Critical tests failed"
          exit 1
        fi